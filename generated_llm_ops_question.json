{
  "question": "How would you design a multi-region inference server deployment that optimizes for both latency and cost when serving LLM models across GPUs and TPUs?",
  "answer": "Use regional model sharding with GPU for general workloads, TPU for transformer-specific tasks, and smart routing based on request patterns.",
  "explanation": "## Why Asked\nTests understanding of distributed ML infrastructure, hardware optimization, and cost-aware architecture design for production LLM services.\n\n## Key Concepts\n- Model parallelism vs data parallelism\n- Hardware-specific optimizations (GPU vs TPU)\n- Latency-aware routing algorithms\n- Auto-scaling strategies for inference\n- Cost optimization through workload distribution\n\n## Code Example\n```\n// Smart routing logic
function routeInference(request) {\n  if (request.modelType === 'transformer' && request.batchSize > 32) {\n    return 'tpu-cluster'; // TPU excels at large batch transformers\n  }\n  if (request.latencyRequirement < 50ms) {\n    return 'gpu-cluster-nearest'; // GPU for low latency\n  }\n  return 'cost-optimized-cluster';\n}\n```\n\n## Follow-up Questions\n- How would you handle model versioning across regions?\n- What monitoring metrics would you track for performance?\n- How do you ensure high availability during hardware failures?",
  "diagram": "flowchart TD\n  A[Client Request] --> B{Analyze Request}\n  B -->|Low Latency| C[GPU Cluster - Nearest Region]\n  B -->|Large Batch Transformer| D[TPU Cluster - Optimized Region]\n  B -->|Cost Sensitive| E[GPU Cluster - Cost Region]\n  C --> F[Load Balancer]\n  D --> F\n  E --> F\n  F --> G[Model Server]\n  G --> H[Response]",
  "companies": ["Google", "Amazon", "Meta", "Microsoft", "NVIDIA"],
  "sourceUrl": "https://cloud.google.com/tpu/docs/inference-optimization",
  "videos": {"shortVideo": null, "longVideo": null}
}