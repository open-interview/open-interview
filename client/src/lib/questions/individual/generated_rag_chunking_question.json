{
  "question": "How would you implement chunking strategies for optimal RAG retrieval when dealing with documents of varying lengths and structures?",
  "answer": "Use semantic chunking with overlapping windows, adaptive sizing based on content type, and metadata preservation for context.",
  "explanation": "## Why Asked\nTests understanding of RAG preprocessing optimization and practical implementation challenges.\n\n## Key Concepts\n- Semantic chunking vs fixed-size chunking\n- Overlapping windows to maintain context\n- Adaptive chunking based on document structure\n- Metadata preservation for retrieval relevance\n- Embedding optimization for different chunk sizes\n\n## Code Example\n```\nfunction semanticChunking(text, maxTokens=512, overlap=50) {\n  const sentences = text.split('. ');\n  const chunks = [];\n  let currentChunk = '';\n  \n  for (const sentence of sentences) {\n    if (getTokenCount(currentChunk + sentence) > maxTokens) {\n      chunks.push(currentChunk.trim());\n      currentChunk = sentences.slice(\n        Math.max(0, sentences.indexOf(sentence) - overlap)\n      ).join('. ');\n    }\n    currentChunk += sentence + '. ';\n  }\n  \n  if (currentChunk.trim()) chunks.push(currentChunk.trim());\n  return chunks;\n}\n```\n\n## Follow-up Questions\n- How do you handle code blocks or tables in chunking?\n- What metrics would you use to evaluate chunking effectiveness?\n- How would you implement recursive character splitting?",
  "diagram": "flowchart TD\n  A[Input Document] --> B{Document Type?}\n  B -->|Code| C[Syntax-Aware Splitting]\n  B -->|Text| D[Semantic Chunking]\n  B -->|Table| E[Row-Based Splitting]\n  C --> F[Add Overlap]\n  D --> F\n  E --> F\n  F --> G[Generate Embeddings]\n  G --> H[Store in Vector DB]\n  H --> I[Index with Metadata]",
  "companies": ["Google", "Amazon", "Meta"],
  "sourceUrl": "https://docs.langchain.com/docs/modules/data_connection/document_transformers/",
  "videos": {"shortVideo": null, "longVideo": null}
}