[
  {
    "id": "db-1",
    "question": "What is the difference between Clustered and Non-Clustered Indexes?",
    "answer": "Clustered Index defines the physical order of data. Non-Clustered is a separate lookup structure.",
    "explanation": "**Clustered Index**:\n- Only 1 per table (usually Primary Key).\n- Leaf nodes contain the ACTUAL data rows.\n- Faster for range queries.\n\n**Non-Clustered Index**:\n- Multiple allowed.\n- Leaf nodes contain pointers to the data (row ID or clustered key).\n- Requires 'Bookmark Lookup' (extra hop) to get full data.",
    "tags": [
      "sql",
      "indexing",
      "perf"
    ],
    "difficulty": "beginner",
    "channel": "database",
    "subChannel": "sql",
    "diagram": "graph TD\n    subgraph Clustered\n    Root1 --> Data[\"Data Pages<br/>Sorted\"]\n    end\n    subgraph NonClustered\n    Root2 --> Ptr[Pointers]\n    Ptr -.-> Data\n    end",
    "lastUpdated": "2025-12-12T09:07:04.186Z"
  },
  {
    "id": "db-2",
    "question": "How do ACID properties ensure data integrity in a banking transaction where $100 is transferred from Account A to Account B?",
    "answer": "Atomicity ensures all-or-nothing execution, Consistency maintains valid states, Isolation prevents interference, Durability guarantees persistence.",
    "explanation": "**Banking Transfer Scenario**: Account A transfers $100 to Account B\n\n• **Atomicity**: Either both debit from A AND credit to B succeed, or both fail and rollback completely\n• **Consistency**: Database maintains valid state - total money remains constant, account balances never go negative\n• **Isolation**: Concurrent transactions see consistent snapshots - if Account C checks A's balance during transfer, they see either before or after state, never partial\n• **Durability**: Once transaction commits, changes persist even through system crashes via Write-Ahead Logging",
    "tags": [
      "acid",
      "transactions",
      "theory"
    ],
    "difficulty": "intermediate",
    "channel": "database",
    "subChannel": "transactions",
    "diagram": "graph TD\n    Start[Transfer $100: A → B] --> Atomic{Atomicity Check}\n    Atomic -->|Success| Consistent[Consistency Validation]\n    Atomic -->|Failure| Rollback[Complete Rollback]\n    Consistent --> Isolated[Isolation Control]\n    Isolated --> Durable[Durability Commit]\n    Durable --> Complete[Transaction Complete]\n    Rollback --> Failed[Transaction Failed]",
    "lastUpdated": "2025-12-12T09:08:19.686Z"
  },
  {
    "id": "gh-67",
    "question": "How does Database DevOps integrate database schema changes into CI/CD pipelines while ensuring data integrity and minimizing downtime?",
    "answer": "Database DevOps integrates database changes into automated pipelines using version-controlled migrations, automated testing, and staged deployment strategies.",
    "explanation": "Database DevOps applies DevOps principles to database development, enabling safe, automated database change management.\n\n## Core Practices:\n• **Version Control**: Store migration scripts in Git for complete change history\n• **Automated Testing**: Validate schemas, test data migrations, verify performance\n• **Staged Deployments**: Use blue-green or canary deployments for database changes\n• **Rollback Procedures**: Automated rollback scripts for failed changes\n• **Monitoring**: Track database performance and change impact\n\n## CI/CD Integration:\n- **Continuous Integration**: Automated schema validation and testing\n- **Continuous Delivery**: Staged deployment with automated rollback\n- **Database Versioning**: Sequential migration scripts with version tracking\n- **Environment Synchronization**: Consistent environments across dev/staging/prod",
    "tags": [
      "db",
      "devops"
    ],
    "difficulty": "beginner",
    "channel": "database",
    "subChannel": "fundamentals",
    "lastUpdated": "2025-12-12T09:11:16.660Z",
    "diagram": "graph TD\n    Dev[Developer] --> VC[Version Control]\n    VC --> CI[CI Pipeline]\n    CI --> Schema[Schema Validation]\n    CI --> Test[Automated Tests]\n    Schema --> Build[Build Artifacts]\n    Test --> Build\n    Build --> CD[CD Pipeline]\n    CD --> Stage[Staging Deploy]\n    Stage --> Verify[Data Verification]\n    Verify --> Prod[Production Deploy]\n    Prod --> Monitor[Database Monitoring]\n    Monitor --> Alert{Issues?}\n    Alert -->|Yes| Rollback[Automated Rollback]\n    Alert -->|No| Success[Deploy Success]\n    Rollback --> CD"
  },
  {
    "id": "da-125",
    "question": "Explain database indexing and when should you use it?",
    "answer": "Database indexes are data structures that improve query speed by maintaining sorted references to data locations, trading increased write overhead for faster read operations.",
    "explanation": "**How Indexes Work**:\n- Create sorted data structures (B-tree, Hash) that point to actual data\n- Enable efficient data location without full table scans\n- Trade write performance for read speed improvements\n\n**When to Use**:\n- Frequently queried columns\n- WHERE clause conditions\n- JOIN operations\n- ORDER BY and GROUP BY columns\n\n**When NOT to Use**:\n- Small tables (< 100 rows)\n- Frequently updated columns\n- Low cardinality columns\n- Write-heavy workloads",
    "tags": [
      "sql",
      "indexing"
    ],
    "difficulty": "intermediate",
    "channel": "database",
    "subChannel": "sql",
    "diagram": "graph TD\n    Query[SQL Query] --> Index[(Database Index)]\n    Index --> DataPoint[Data Location Pointer]\n    DataPoint --> TableData[(Table Data)]\n    WriteOp[Write Operation] --> IndexUpdate[Update Index]\n    IndexUpdate --> TableUpdate[Update Table]\n    style Index fill:#4ade80\n    style TableData fill:#fbbf24",
    "lastUpdated": "2025-12-12T09:11:28.015Z"
  },
  {
    "id": "da-128",
    "question": "You have a banking system where users can transfer money between accounts. Design a transaction to handle a transfer of $500 from Account A (balance: $1000) to Account B (balance: $200). What happens if the system crashes after debiting Account A but before crediting Account B? How would you ensure data consistency?",
    "answer": "Use database transactions with ACID properties. Wrap both operations in a single transaction that either commits both or rolls back both.",
    "explanation": "## Database Transaction for Money Transfer\n\nThis scenario illustrates the critical importance of **ACID properties** in database transactions:\n\n### The Problem\nWithout proper transaction handling:\n1. Debit $500 from Account A (balance becomes $500)\n2. **System crashes here**\n3. Credit to Account B never happens\n4. **Result: $500 disappears from the system**\n\n### The Solution: ACID Transaction\n\n```sql\nBEGIN TRANSACTION;\n\n-- Check sufficient funds\nSELECT balance FROM accounts WHERE id = 'A' FOR UPDATE;\n\n-- Perform both operations atomically\nUPDATE accounts SET balance = balance - 500 WHERE id = 'A';\nUPDATE accounts SET balance = balance + 500 WHERE id = 'B';\n\nCOMMIT;\n```\n\n### ACID Properties Explained\n\n- **Atomicity**: Both operations succeed together or fail together\n- **Consistency**: Total money in system remains constant\n- **Isolation**: Other transactions can't see intermediate states\n- **Durability**: Once committed, changes survive system crashes\n\n### Additional Safeguards\n\n1. **Deadlock Prevention**: Always acquire locks in consistent order (e.g., by account ID)\n2. **Timeout Handling**: Set transaction timeouts to prevent indefinite locks\n3. **Retry Logic**: Implement exponential backoff for transient failures\n4. **Audit Trail**: Log all transaction attempts for reconciliation",
    "tags": [
      "acid",
      "transactions"
    ],
    "difficulty": "intermediate",
    "channel": "database",
    "subChannel": "transactions",
    "diagram": "graph TD\n    A[Start Transaction] --> B[Lock Account A]\n    B --> C[Check Balance >= $500]\n    C -->|Yes| D[Debit $500 from Account A]\n    C -->|No| E[Rollback - Insufficient Funds]\n    D --> F[Credit $500 to Account B]\n    F --> G[Commit Transaction]\n    G --> H[Release Locks]\n    \n    D -->|System Crash| I[Automatic Rollback]\n    F -->|System Crash| I\n    I --> J[Both Accounts Restored]\n    \n    E --> K[Transaction Failed]\n    \n    style A fill:#e1f5fe\n    style G fill:#c8e6c9\n    style I fill:#ffcdd2\n    style E fill:#ffcdd2",
    "lastUpdated": "2025-12-12T09:07:04.186Z"
  },
  {
    "id": "da-129",
    "question": "What is the main difference between SQL and NoSQL databases in terms of data structure?",
    "answer": "SQL uses structured tables with fixed schemas, NoSQL uses flexible document/key-value/graph structures without fixed schemas.",
    "explanation": "## SQL vs NoSQL Data Structure\n\n**SQL Databases:**\n- Store data in **tables** with rows and columns\n- Require a **predefined schema** (structure must be defined before inserting data)\n- Data must conform to the schema (same columns for all rows)\n- Examples: MySQL, PostgreSQL, Oracle\n\n**NoSQL Databases:**\n- Store data in flexible formats:\n  - **Document stores** (JSON-like documents) - MongoDB, CouchDB\n  - **Key-value pairs** - Redis, DynamoDB\n  - **Column-family** - Cassandra, HBase\n  - **Graph databases** - Neo4j, Amazon Neptune\n- **Schema-less** or **schema-flexible**\n- Can store different structures in the same collection/table\n- Better for rapidly changing requirements\n\n**When to use NoSQL:**\n- Rapidly evolving data structures\n- Large scale applications requiring horizontal scaling\n- Semi-structured or unstructured data\n- Real-time applications",
    "tags": [
      "nosql",
      "mongodb"
    ],
    "difficulty": "beginner",
    "channel": "database",
    "subChannel": "nosql",
    "diagram": "graph TD\n    A[Database Types] --> B[SQL Databases]\n    A --> C[NoSQL Databases]\n    \n    B --> D[Fixed Schema]\n    B --> E[Tables with Rows/Columns]\n    B --> F[ACID Compliance]\n    \n    C --> G[Flexible Schema]\n    C --> H[Multiple Data Models]\n    \n    H --> I[Document Store]\n    H --> J[Key-Value]\n    H --> K[Column-Family]\n    H --> L[Graph]\n    \n    I --> M[MongoDB]\n    J --> N[Redis]\n    K --> O[Cassandra]\n    L --> P[Neo4j]",
    "lastUpdated": "2025-12-12T09:07:04.186Z"
  },
  {
    "id": "da-134",
    "question": "You have a banking system where Account A transfers $100 to Account B, but during the transaction, Account B gets deleted by another process. The transfer uses READ COMMITTED isolation. What happens to the $100, and how would you prevent data inconsistency?",
    "answer": "Money disappears into deleted account. Use SELECT FOR UPDATE or SERIALIZABLE isolation to prevent phantom reads and ensure referential integrity.",
    "explanation": "## Transaction Isolation and Phantom Reads\n\nThis scenario demonstrates a **phantom read** problem in READ COMMITTED isolation:\n\n### What Happens:\n1. **Transaction T1** (transfer): Reads Account A balance, debits $100\n2. **Transaction T2** (deletion): Deletes Account B \n3. **Transaction T1**: Attempts to credit Account B - but it no longer exists\n4. **Result**: $100 vanishes from the system\n\n### Why READ COMMITTED Fails:\n- Only prevents **dirty reads** and **non-repeatable reads**\n- Does **NOT** prevent **phantom reads** (rows appearing/disappearing)\n- Account B's existence isn't locked during the transfer\n\n### Solutions:\n\n#### 1. Row-Level Locking\n```sql\nBEGIN;\nSELECT balance FROM accounts WHERE id = 'B' FOR UPDATE;\n-- This locks Account B, preventing deletion\nUPDATE accounts SET balance = balance - 100 WHERE id = 'A';\nUPDATE accounts SET balance = balance + 100 WHERE id = 'B';\nCOMMIT;\n```\n\n#### 2. SERIALIZABLE Isolation\n```sql\nSET TRANSACTION ISOLATION LEVEL SERIALIZABLE;\n-- Prevents all anomalies including phantom reads\n```\n\n#### 3. Application-Level Validation\n```sql\nBEGIN;\nIF NOT EXISTS (SELECT 1 FROM accounts WHERE id = 'B') THEN\n    ROLLBACK;\nEND IF;\n-- Proceed with transfer\nCOMMIT;\n```\n\n### Best Practice:\nUse **SELECT FOR UPDATE** on target accounts before any transfer to ensure atomicity and prevent phantom deletions.",
    "tags": [
      "acid",
      "transactions"
    ],
    "difficulty": "advanced",
    "channel": "database",
    "subChannel": "transactions",
    "diagram": "graph TD\n    A[Transaction T1: Transfer $100] --> B[Read Account A: $500]\n    A --> C[Read Account B: $200]\n    D[Transaction T2: Delete Account B] --> E[DELETE FROM accounts WHERE id='B']\n    B --> F[UPDATE Account A: $400]\n    C --> G[UPDATE Account B: ???]\n    E --> H[Account B Deleted]\n    G --> I[ERROR: Account B not found]\n    F --> J[Money Lost: $100 vanished]\n    \n    K[Solution: SELECT FOR UPDATE] --> L[Lock Account B]\n    L --> M[Prevent Deletion]\n    M --> N[Safe Transfer]",
    "lastUpdated": "2025-12-12T09:14:57.084Z"
  },
  {
    "id": "da-145",
    "question": "You have a table `orders` with columns (id, user_id, amount, created_at) and need to find users who made their first purchase in the last 30 days AND have made at least 3 purchases total, but their average order value is below the overall platform average. Write an optimized SQL query.",
    "answer": "Use window functions with ROW_NUMBER() to find first purchases, COUNT() for total orders, and subqueries for average comparisons with proper indexing.",
    "explanation": "## Solution\n\n```sql\nWITH user_stats AS (\n  SELECT \n    user_id,\n    COUNT(*) as total_orders,\n    AVG(amount) as user_avg_amount,\n    MIN(created_at) as first_order_date\n  FROM orders\n  GROUP BY user_id\n  HAVING COUNT(*) >= 3\n),\nplatform_avg AS (\n  SELECT AVG(amount) as overall_avg\n  FROM orders\n)\nSELECT DISTINCT us.user_id\nFROM user_stats us\nCROSS JOIN platform_avg pa\nWHERE us.first_order_date >= CURRENT_DATE - INTERVAL '30 days'\n  AND us.user_avg_amount < pa.overall_avg;\n```\n\n## Key Concepts\n\n- **CTEs (Common Table Expressions)**: Break complex logic into readable chunks\n- **Window Functions**: Efficient for ranking and aggregations\n- **Performance Optimization**: \n  - Index on `(user_id, created_at)` for grouping\n  - Index on `created_at` for date filtering\n  - HAVING clause filters before final result set\n- **Cross Join**: Efficiently compare user averages to platform average\n\n## Alternative Approach\n```sql\nSELECT user_id\nFROM orders o1\nWHERE (SELECT MIN(created_at) FROM orders o2 WHERE o2.user_id = o1.user_id) >= CURRENT_DATE - INTERVAL '30 days'\nGROUP BY user_id\nHAVING COUNT(*) >= 3\n  AND AVG(amount) < (SELECT AVG(amount) FROM orders);\n```\n\nThis tests understanding of aggregation functions, subqueries, date operations, and query optimization strategies.",
    "tags": [
      "sql",
      "indexing"
    ],
    "difficulty": "advanced",
    "channel": "database",
    "subChannel": "sql",
    "diagram": "graph TD\n    A[orders table] --> B[Group by user_id]\n    B --> C[Calculate user stats]\n    C --> D[Filter: total_orders >= 3]\n    D --> E[Filter: first_order in last 30 days]\n    E --> F[Compare user_avg < platform_avg]\n    F --> G[Return qualifying user_ids]\n    \n    H[Platform Average] --> F\n    \n    style A fill:#e1f5fe\n    style G fill:#c8e6c9\n    style F fill:#fff3e0",
    "lastUpdated": "2025-12-12T10:05:38.475Z"
  },
  {
    "id": "da-156",
    "question": "What is the difference between DELETE and TRUNCATE commands in SQL?",
    "answer": "DELETE removes rows one by one and can use WHERE; TRUNCATE removes all rows at once, faster but can't be filtered.",
    "explanation": "## DELETE vs TRUNCATE\n\n### DELETE Command\n- Removes rows one at a time\n- Can use WHERE clause to filter specific rows\n- Triggers are fired for each deleted row\n- Slower for large datasets\n- Transaction log records each row deletion\n- Can be rolled back\n\n```sql\nDELETE FROM users WHERE age < 18;\n```\n\n### TRUNCATE Command\n- Removes all rows at once\n- Cannot use WHERE clause (removes entire table data)\n- No triggers fired\n- Much faster for large datasets\n- Minimal transaction logging\n- Cannot be rolled back in most databases\n- Resets auto-increment counters\n\n```sql\nTRUNCATE TABLE users;\n```\n\n### When to Use Each\n- Use **DELETE** when you need to remove specific rows or need transaction safety\n- Use **TRUNCATE** when you need to quickly empty an entire table",
    "tags": [
      "sql",
      "indexing"
    ],
    "difficulty": "beginner",
    "channel": "database",
    "subChannel": "sql",
    "diagram": "graph TD\n    A[SQL Data Removal] --> B[DELETE]\n    A --> C[TRUNCATE]\n    B --> D[Row-by-row removal]\n    B --> E[WHERE clause supported]\n    B --> F[Slower, logged]\n    B --> G[Can rollback]\n    C --> H[Bulk removal]\n    C --> I[No WHERE clause]\n    C --> J[Faster, minimal log]\n    C --> K[Cannot rollback]\n    C --> L[Resets auto-increment]",
    "lastUpdated": "2025-12-13T01:08:56.357Z"
  },
  {
    "id": "da-172",
    "question": "In a distributed database system, how would you implement a two-phase commit protocol to ensure atomicity across multiple nodes, and what are the key failure scenarios you must handle?",
    "answer": "Use coordinator to prepare/commit phases, handle node failures, timeouts, and network partitions with recovery protocols.",
    "explanation": "## Two-Phase Commit Implementation\n\n### Phase 1: Prepare\n1. **Coordinator sends prepare** to all participants\n2. **Participants validate** they can commit (lock resources, write to log)\n3. **Participants respond** with 'vote-commit' or 'vote-abort'\n\n### Phase 2: Commit\n- If all vote-commit: coordinator sends commit, participants acknowledge\n- If any vote-abort: coordinator sends abort, participants rollback\n\n### Failure Scenarios\n1. **Coordinator failure during prepare**: Participants timeout and abort\n2. **Coordinator failure after decision**: Use transaction logs to recover\n3. **Participant failure**: Coordinator retries, other participants wait\n4. **Network partition**: Timeout mechanisms prevent indefinite blocking\n\n### Optimizations\n- **Three-phase commit** adds pre-commit phase to reduce blocking\n- **Paxos/Raft** for coordinator election\n- **Timeout handling** with exponential backoff",
    "tags": [
      "acid",
      "transactions"
    ],
    "difficulty": "advanced",
    "channel": "database",
    "subChannel": "transactions",
    "diagram": "graph TD\n    A[Coordinator] -->|Prepare Request| B[Participant 1]\n    A -->|Prepare Request| C[Participant 2]\n    A -->|Prepare Request| D[Participant 3]\n    B -->|Vote-Commit| A\n    C -->|Vote-Commit| A\n    D -->|Vote-Commit| A\n    A -->|Global Commit| B\n    A -->|Global Commit| C\n    A -->|Global Commit| D\n    B -->|Ack| A\n    C -->|Ack| A\n    D -->|Ack| A\n    E[Failure Scenario] --> F[Coordinator Crash]\n    E --> G[Participant Timeout]\n    E --> H[Network Partition]",
    "lastUpdated": "2025-12-14T01:18:08.909Z"
  },
  {
    "id": "da-170",
    "question": "You're building a banking system where users can transfer money between accounts. How would you design the transaction handling to ensure no money is lost or created during transfers, especially when the system crashes mid-transfer?",
    "answer": "Use ACID transactions with BEGIN, UPDATE accounts, COMMIT/ROLLBACK. Ensure atomicity by debiting source and crediting destination in single transaction.",
    "explanation": "## Transaction Design for Money Transfers\n\n### Key Requirements\n- **Atomicity**: Both debit and credit must succeed or fail together\n- **Consistency**: Total money in system remains constant\n- **Isolation**: Concurrent transfers don't interfere\n- **Durability**: Completed transfers survive system crashes\n\n### Implementation Strategy\n\n```sql\nBEGIN TRANSACTION;\n\n-- Check sufficient funds\nSELECT balance FROM accounts WHERE id = :source_id FOR UPDATE;\n\n-- Debit source account\nUPDATE accounts \nSET balance = balance - :amount \nWHERE id = :source_id AND balance >= :amount;\n\n-- Credit destination account  \nUPDATE accounts \nSET balance = balance + :amount \nWHERE id = :dest_id;\n\n-- Verify both operations succeeded\nIF (rowcount_source = 1 AND rowcount_dest = 1) THEN\n    COMMIT;\nELSE\n    ROLLBACK;\nEND IF;\n```\n\n### Crash Recovery\n- **Before COMMIT**: Transaction is rolled back on restart\n- **After COMMIT**: Changes are durable due to write-ahead logging\n- **During COMMIT**: Database ensures atomic completion\n\n### Concurrency Control\n- Use row-level locks with `FOR UPDATE`\n- Implement deadlock detection and retry logic\n- Consider isolation levels (READ COMMITTED vs SERIALIZABLE)\n\n### Monitoring\n- Track transaction success/failure rates\n- Monitor lock contention and deadlock frequency\n- Audit trail for all financial transactions",
    "tags": [
      "acid",
      "transactions"
    ],
    "difficulty": "intermediate",
    "channel": "database",
    "subChannel": "transactions",
    "diagram": "graph TD\n    A[Client Initiates Transfer] --> B[BEGIN TRANSACTION]\n    B --> C[LOCK Source Account]\n    C --> D[CHECK Balance >= Amount]\n    D --> E{Sufficient Funds?}\n    E -->|No| F[ROLLBACK - Return Error]\n    E -->|Yes| G[DEBIT Source Account]\n    G --> H[LOCK Destination Account]\n    H --> I[CREDIT Destination Account]\n    I --> J{Both Updates Successful?}\n    J -->|No| K[ROLLBACK - Return Error]\n    J -->|Yes| L[COMMIT Transaction]\n    L --> M[Release Locks]\n    M --> N[Return Success]\n    \n    O[System Crash] --> P{Crash Timing}\n    P -->|Before COMMIT| Q[Automatic ROLLBACK on Restart]\n    P -->|After COMMIT| R[Changes Preserved via WAL]\n    P -->|During COMMIT| S[Database Ensures Atomic Completion]",
    "lastUpdated": "2025-12-14T01:18:46.794Z"
  }
]