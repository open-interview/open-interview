{
  "question": "How does LSTM solve the vanishing gradient problem compared to vanilla RNNs?",
  "answer": "LSTM uses gates (input, forget, output) and cell state to control information flow, preserving gradients over long sequences.",
  "explanation": "## Why Asked\nTests understanding of core RNN limitations and LSTM's architectural innovations for sequence modeling.\n\n## Key Concepts\n- Vanishing/exploding gradients in deep RNNs\n- Cell state as information highway\n- Gate mechanisms for selective memory\n- Gradient flow through additive connections\n\n## Code Example\n```python\nclass LSTMCell:\n    def __init__(self, input_size, hidden_size):\n        self.W_f = torch.randn(input_size + hidden_size, hidden_size)\n        self.W_i = torch.randn(input_size + hidden_size, hidden_size)\n        self.W_o = torch.randn(input_size + hidden_size, hidden_size)\n        self.W_c = torch.randn(input_size + hidden_size, hidden_size)\n    \n    def forward(self, x, h_prev, c_prev):\n        combined = torch.cat([x, h_prev], dim=1)\n        f = torch.sigmoid(combined @ self.W_f)  # forget gate\n        i = torch.sigmoid(combined @ self.W_i)  # input gate\n        o = torch.sigmoid(combined @ self.W_o)  # output gate\n        c_tilde = torch.tanh(combined @ self.W_c)  # candidate\n        c = f * c_prev + i * c_tilde  # cell state (additive!)
n        h = o * torch.tanh(c)  # hidden state\n        return h, c\n```\n\n## Follow-up Questions\n- When would you prefer GRU over LSTM?\n- How do bidirectional LSTMs work?\n- What are attention mechanisms and how do they enhance sequence models?",
  "diagram": "flowchart TD\n    A[Input x_t] --> B[Concatenate with h_{t-1}]\n    B --> C[Forget Gate f_t]\n    B --> D[Input Gate i_t]\n    B --> E[Output Gate o_t]\n    B --> F[Candidate Memory c̃_t]\n    C --> G[Cell State c_t = f_t⊙c_{t-1} + i_t⊙c̃_t]\n    D --> G\n    F --> G\n    G --> H[Hidden State h_t = o_t⊙tanh(c_t)]\n    E --> H\n    H --> I[Output/Next Input]",
  "companies": ["Google", "Amazon", "Meta", "Microsoft", "Apple"],
  "sourceUrl": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/",
  "videos": {"shortVideo": null, "longVideo": null}
}